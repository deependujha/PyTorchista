{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n",
    "\n",
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 5.0\n",
      "y: 45.0\n",
      "gradient of x: None\n"
     ]
    }
   ],
   "source": [
    "# let x = 5\n",
    "# and, y = x**2 + 3*x + 5\n",
    "\n",
    "# then, dy/dx at x=5:\n",
    "#   dy/dx = 2*x + 3\n",
    "#   dy/dx (at x=5) = 2*5+3 = 13\n",
    "#   dy/dx is called gradient of y w.r.t. x, & in Pytorch, we will use `x.grad` for it.\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(5.0, dtype=torch.float32, requires_grad=True)\n",
    "y = x**2 + 3 * x + 5\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "print(f\"gradient of x: {x.grad}\")\n",
    "\n",
    "# print(f\"gradient of y: {y.grad}\")\n",
    "# we find gradients on the leaf node.\n",
    "# that's why we don't have leaf_node.grad and accessing will give you warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # start differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad: 27.0\n",
      "b.grad: 10.0\n"
     ]
    }
   ],
   "source": [
    "# backward() function computes partial differentiation w.r.t to variables present in the computation graph.\n",
    "\n",
    "# let's take another example:\n",
    "a = torch.tensor(3.0, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(5.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "c = a**3 + b**2 + 10\n",
    "# a.grad should be: 3*(a**2) = 27\n",
    "# b.grad should be: 2*b = 10\n",
    "\n",
    "c.backward()\n",
    "print(f\"a.grad: {a.grad}\")\n",
    "print(f\"b.grad: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- The backward() & grad are the ones which are used in the automatic differentiation in neural networks in PyTorch.\n",
    "\n",
    "![automatic differentiation](../00_assets/Important/computational_graph.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
